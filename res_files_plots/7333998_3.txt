training cnn for DL
params.use_seqs:  True
Init. encoder...
Loading w2v embeddings...
Initialise Datasets...
Done.
Initialise DataLoaders...
Done.
Start training...
Train Epoch: 1/30 [0/269796 (0%)]	Training loss for batch 1/4216: 0.694428
Train Epoch: 1/30 [6400/269796 (2%)]	Training loss for batch 101/4216: 0.099512
Train Epoch: 1/30 [12800/269796 (5%)]	Training loss for batch 201/4216: 0.090064
Train Epoch: 1/30 [19200/269796 (7%)]	Training loss for batch 301/4216: 0.098879
Train Epoch: 1/30 [25600/269796 (9%)]	Training loss for batch 401/4216: 0.095425
Train Epoch: 1/30 [32000/269796 (12%)]	Training loss for batch 501/4216: 0.103777
Train Epoch: 1/30 [38400/269796 (14%)]	Training loss for batch 601/4216: 0.099662
Train Epoch: 1/30 [44800/269796 (17%)]	Training loss for batch 701/4216: 0.092033
Train Epoch: 1/30 [51200/269796 (19%)]	Training loss for batch 801/4216: 0.091479
Train Epoch: 1/30 [57600/269796 (21%)]	Training loss for batch 901/4216: 0.093456

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0860, Precision: 45.94%, Recall: 14.31%, F1: 21.79%, Acc: 97.39%

Train Epoch: 1/30 [64000/269796 (24%)]	Training loss for batch 1001/4216: 0.081426
Train Epoch: 1/30 [70400/269796 (26%)]	Training loss for batch 1101/4216: 0.087075
Train Epoch: 1/30 [76800/269796 (28%)]	Training loss for batch 1201/4216: 0.094247
Train Epoch: 1/30 [83200/269796 (31%)]	Training loss for batch 1301/4216: 0.088600
Train Epoch: 1/30 [89600/269796 (33%)]	Training loss for batch 1401/4216: 0.078377
Train Epoch: 1/30 [96000/269796 (36%)]	Training loss for batch 1501/4216: 0.088014
Train Epoch: 1/30 [102400/269796 (38%)]	Training loss for batch 1601/4216: 0.086467
Train Epoch: 1/30 [108800/269796 (40%)]	Training loss for batch 1701/4216: 0.092476
Train Epoch: 1/30 [115200/269796 (43%)]	Training loss for batch 1801/4216: 0.083737
Train Epoch: 1/30 [121600/269796 (45%)]	Training loss for batch 1901/4216: 0.087463

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0849, Precision: 51.94%, Recall: 4.93%, F1: 8.83%, Acc: 97.49%

Train Epoch: 1/30 [128000/269796 (47%)]	Training loss for batch 2001/4216: 0.086195
Train Epoch: 1/30 [134400/269796 (50%)]	Training loss for batch 2101/4216: 0.079229
Train Epoch: 1/30 [140800/269796 (52%)]	Training loss for batch 2201/4216: 0.086219
Train Epoch: 1/30 [147200/269796 (55%)]	Training loss for batch 2301/4216: 0.082839
Train Epoch: 1/30 [153600/269796 (57%)]	Training loss for batch 2401/4216: 0.075790
Train Epoch: 1/30 [160000/269796 (59%)]	Training loss for batch 2501/4216: 0.073543
Train Epoch: 1/30 [166400/269796 (62%)]	Training loss for batch 2601/4216: 0.083474
Train Epoch: 1/30 [172800/269796 (64%)]	Training loss for batch 2701/4216: 0.081637
Train Epoch: 1/30 [179200/269796 (66%)]	Training loss for batch 2801/4216: 0.078460
Train Epoch: 1/30 [185600/269796 (69%)]	Training loss for batch 2901/4216: 0.092573

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0841, Precision: 51.83%, Recall: 6.01%, F1: 10.57%, Acc: 97.49%

Train Epoch: 1/30 [192000/269796 (71%)]	Training loss for batch 3001/4216: 0.083726
Train Epoch: 1/30 [198400/269796 (74%)]	Training loss for batch 3101/4216: 0.085224
Train Epoch: 1/30 [204800/269796 (76%)]	Training loss for batch 3201/4216: 0.085036
Train Epoch: 1/30 [211200/269796 (78%)]	Training loss for batch 3301/4216: 0.083307
Train Epoch: 1/30 [217600/269796 (81%)]	Training loss for batch 3401/4216: 0.079886
Train Epoch: 1/30 [224000/269796 (83%)]	Training loss for batch 3501/4216: 0.083886
Train Epoch: 1/30 [230400/269796 (85%)]	Training loss for batch 3601/4216: 0.093572
Train Epoch: 1/30 [236800/269796 (88%)]	Training loss for batch 3701/4216: 0.080299
Train Epoch: 1/30 [243200/269796 (90%)]	Training loss for batch 3801/4216: 0.080489
Train Epoch: 1/30 [249600/269796 (93%)]	Training loss for batch 3901/4216: 0.083440

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0860, Precision: 56.91%, Recall: 12.26%, F1: 20.07%, Acc: 97.54%

Train Epoch: 1/30 [256000/269796 (95%)]	Training loss for batch 4001/4216: 0.089909
Train Epoch: 1/30 [262400/269796 (97%)]	Training loss for batch 4101/4216: 0.083594
Train Epoch: 1/30 [268800/269796 (100%)]	Training loss for batch 4201/4216: 0.083551

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0820, Precision: 58.97%, Recall: 9.83%, F1: 16.48%, Acc: 97.56%

Train Epoch: 2/30 [0/269796 (0%)]	Training loss for batch 1/4216: 0.085326
Train Epoch: 2/30 [6400/269796 (2%)]	Training loss for batch 101/4216: 0.086441
Train Epoch: 2/30 [12800/269796 (5%)]	Training loss for batch 201/4216: 0.091425
Train Epoch: 2/30 [19200/269796 (7%)]	Training loss for batch 301/4216: 0.077348
Train Epoch: 2/30 [25600/269796 (9%)]	Training loss for batch 401/4216: 0.081773
Train Epoch: 2/30 [32000/269796 (12%)]	Training loss for batch 501/4216: 0.088915
Train Epoch: 2/30 [38400/269796 (14%)]	Training loss for batch 601/4216: 0.086538
Train Epoch: 2/30 [44800/269796 (17%)]	Training loss for batch 701/4216: 0.086508
Train Epoch: 2/30 [51200/269796 (19%)]	Training loss for batch 801/4216: 0.087816
Train Epoch: 2/30 [57600/269796 (21%)]	Training loss for batch 901/4216: 0.092086

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0801, Precision: 61.71%, Recall: 13.63%, F1: 21.48%, Acc: 97.62%

Train Epoch: 2/30 [64000/269796 (24%)]	Training loss for batch 1001/4216: 0.081534
Train Epoch: 2/30 [70400/269796 (26%)]	Training loss for batch 1101/4216: 0.087863
Train Epoch: 2/30 [76800/269796 (28%)]	Training loss for batch 1201/4216: 0.079340
Train Epoch: 2/30 [83200/269796 (31%)]	Training loss for batch 1301/4216: 0.074886
Train Epoch: 2/30 [89600/269796 (33%)]	Training loss for batch 1401/4216: 0.073676
Train Epoch: 2/30 [96000/269796 (36%)]	Training loss for batch 1501/4216: 0.080890
Train Epoch: 2/30 [102400/269796 (38%)]	Training loss for batch 1601/4216: 0.075642
Train Epoch: 2/30 [108800/269796 (40%)]	Training loss for batch 1701/4216: 0.086343
Train Epoch: 2/30 [115200/269796 (43%)]	Training loss for batch 1801/4216: 0.085096
Train Epoch: 2/30 [121600/269796 (45%)]	Training loss for batch 1901/4216: 0.066894

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0796, Precision: 68.22%, Recall: 14.03%, F1: 22.50%, Acc: 97.67%

Train Epoch: 2/30 [128000/269796 (47%)]	Training loss for batch 2001/4216: 0.084015
Train Epoch: 2/30 [134400/269796 (50%)]	Training loss for batch 2101/4216: 0.081734
Train Epoch: 2/30 [140800/269796 (52%)]	Training loss for batch 2201/4216: 0.075656
Train Epoch: 2/30 [147200/269796 (55%)]	Training loss for batch 2301/4216: 0.071312
Train Epoch: 2/30 [153600/269796 (57%)]	Training loss for batch 2401/4216: 0.087775
Train Epoch: 2/30 [160000/269796 (59%)]	Training loss for batch 2501/4216: 0.083225
Train Epoch: 2/30 [166400/269796 (62%)]	Training loss for batch 2601/4216: 0.077367
Train Epoch: 2/30 [172800/269796 (64%)]	Training loss for batch 2701/4216: 0.069981
Train Epoch: 2/30 [179200/269796 (66%)]	Training loss for batch 2801/4216: 0.066457
Train Epoch: 2/30 [185600/269796 (69%)]	Training loss for batch 2901/4216: 0.072365

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0774, Precision: 70.45%, Recall: 15.90%, F1: 24.92%, Acc: 97.72%

Train Epoch: 2/30 [192000/269796 (71%)]	Training loss for batch 3001/4216: 0.086349
Train Epoch: 2/30 [198400/269796 (74%)]	Training loss for batch 3101/4216: 0.075066
Train Epoch: 2/30 [204800/269796 (76%)]	Training loss for batch 3201/4216: 0.078019
Train Epoch: 2/30 [211200/269796 (78%)]	Training loss for batch 3301/4216: 0.069294
Train Epoch: 2/30 [217600/269796 (81%)]	Training loss for batch 3401/4216: 0.077516
Train Epoch: 2/30 [224000/269796 (83%)]	Training loss for batch 3501/4216: 0.072836
Train Epoch: 2/30 [230400/269796 (85%)]	Training loss for batch 3601/4216: 0.072423
Train Epoch: 2/30 [236800/269796 (88%)]	Training loss for batch 3701/4216: 0.062255
Train Epoch: 2/30 [243200/269796 (90%)]	Training loss for batch 3801/4216: 0.075475
Train Epoch: 2/30 [249600/269796 (93%)]	Training loss for batch 3901/4216: 0.078383

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0738, Precision: 71.24%, Recall: 24.37%, F1: 35.51%, Acc: 97.84%

Train Epoch: 2/30 [256000/269796 (95%)]	Training loss for batch 4001/4216: 0.059493
Train Epoch: 2/30 [262400/269796 (97%)]	Training loss for batch 4101/4216: 0.073028
Train Epoch: 2/30 [268800/269796 (100%)]	Training loss for batch 4201/4216: 0.077550

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0811, Precision: 55.55%, Recall: 21.49%, F1: 30.72%, Acc: 97.59%

Train Epoch: 3/30 [0/269796 (0%)]	Training loss for batch 1/4216: 0.089193
Train Epoch: 3/30 [6400/269796 (2%)]	Training loss for batch 101/4216: 0.067922
Train Epoch: 3/30 [12800/269796 (5%)]	Training loss for batch 201/4216: 0.071587
Train Epoch: 3/30 [19200/269796 (7%)]	Training loss for batch 301/4216: 0.075456
Train Epoch: 3/30 [25600/269796 (9%)]	Training loss for batch 401/4216: 0.077890
Train Epoch: 3/30 [32000/269796 (12%)]	Training loss for batch 501/4216: 0.084947
Train Epoch: 3/30 [38400/269796 (14%)]	Training loss for batch 601/4216: 0.073581
Train Epoch: 3/30 [44800/269796 (17%)]	Training loss for batch 701/4216: 0.072876
Train Epoch: 3/30 [51200/269796 (19%)]	Training loss for batch 801/4216: 0.070140
Train Epoch: 3/30 [57600/269796 (21%)]	Training loss for batch 901/4216: 0.067894

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0716, Precision: 75.58%, Recall: 23.74%, F1: 35.46%, Acc: 97.88%

Train Epoch: 3/30 [64000/269796 (24%)]	Training loss for batch 1001/4216: 0.079126
Train Epoch: 3/30 [70400/269796 (26%)]	Training loss for batch 1101/4216: 0.064276
Train Epoch: 3/30 [76800/269796 (28%)]	Training loss for batch 1201/4216: 0.072785
Train Epoch: 3/30 [83200/269796 (31%)]	Training loss for batch 1301/4216: 0.077418
Train Epoch: 3/30 [89600/269796 (33%)]	Training loss for batch 1401/4216: 0.063637
Train Epoch: 3/30 [96000/269796 (36%)]	Training loss for batch 1501/4216: 0.076025
Train Epoch: 3/30 [102400/269796 (38%)]	Training loss for batch 1601/4216: 0.062005
Train Epoch: 3/30 [108800/269796 (40%)]	Training loss for batch 1701/4216: 0.081865
Train Epoch: 3/30 [115200/269796 (43%)]	Training loss for batch 1801/4216: 0.071948
Train Epoch: 3/30 [121600/269796 (45%)]	Training loss for batch 1901/4216: 0.066003

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0688, Precision: 74.08%, Recall: 28.72%, F1: 40.49%, Acc: 97.95%

Train Epoch: 3/30 [128000/269796 (47%)]	Training loss for batch 2001/4216: 0.075157
Train Epoch: 3/30 [134400/269796 (50%)]	Training loss for batch 2101/4216: 0.066390
Train Epoch: 3/30 [140800/269796 (52%)]	Training loss for batch 2201/4216: 0.066595
Train Epoch: 3/30 [147200/269796 (55%)]	Training loss for batch 2301/4216: 0.073973
Train Epoch: 3/30 [153600/269796 (57%)]	Training loss for batch 2401/4216: 0.068570
Train Epoch: 3/30 [160000/269796 (59%)]	Training loss for batch 2501/4216: 0.070319
Train Epoch: 3/30 [166400/269796 (62%)]	Training loss for batch 2601/4216: 0.065568
Train Epoch: 3/30 [172800/269796 (64%)]	Training loss for batch 2701/4216: 0.071926
Train Epoch: 3/30 [179200/269796 (66%)]	Training loss for batch 2801/4216: 0.063826
Train Epoch: 3/30 [185600/269796 (69%)]	Training loss for batch 2901/4216: 0.074394

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0668, Precision: 73.29%, Recall: 31.83%, F1: 43.68%, Acc: 97.98%

Train Epoch: 3/30 [192000/269796 (71%)]	Training loss for batch 3001/4216: 0.059927
Train Epoch: 3/30 [198400/269796 (74%)]	Training loss for batch 3101/4216: 0.060798
Train Epoch: 3/30 [204800/269796 (76%)]	Training loss for batch 3201/4216: 0.054630
Train Epoch: 3/30 [211200/269796 (78%)]	Training loss for batch 3301/4216: 0.061461
Train Epoch: 3/30 [217600/269796 (81%)]	Training loss for batch 3401/4216: 0.070267
Train Epoch: 3/30 [224000/269796 (83%)]	Training loss for batch 3501/4216: 0.065747
Train Epoch: 3/30 [230400/269796 (85%)]	Training loss for batch 3601/4216: 0.073255
Train Epoch: 3/30 [236800/269796 (88%)]	Training loss for batch 3701/4216: 0.065035
Train Epoch: 3/30 [243200/269796 (90%)]	Training loss for batch 3801/4216: 0.065204
Train Epoch: 3/30 [249600/269796 (93%)]	Training loss for batch 3901/4216: 0.063542

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0660, Precision: 74.91%, Recall: 32.00%, F1: 44.12%, Acc: 98.01%

Train Epoch: 3/30 [256000/269796 (95%)]	Training loss for batch 4001/4216: 0.075082
Train Epoch: 3/30 [262400/269796 (97%)]	Training loss for batch 4101/4216: 0.063913
Train Epoch: 3/30 [268800/269796 (100%)]	Training loss for batch 4201/4216: 0.069298

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0652, Precision: 73.59%, Recall: 34.71%, F1: 46.39%, Acc: 98.03%

Train Epoch: 4/30 [0/269796 (0%)]	Training loss for batch 1/4216: 0.075973
Train Epoch: 4/30 [6400/269796 (2%)]	Training loss for batch 101/4216: 0.062323
Train Epoch: 4/30 [12800/269796 (5%)]	Training loss for batch 201/4216: 0.064486
Train Epoch: 4/30 [19200/269796 (7%)]	Training loss for batch 301/4216: 0.071942
Train Epoch: 4/30 [25600/269796 (9%)]	Training loss for batch 401/4216: 0.077675
Train Epoch: 4/30 [32000/269796 (12%)]	Training loss for batch 501/4216: 0.069128
Train Epoch: 4/30 [38400/269796 (14%)]	Training loss for batch 601/4216: 0.065908
Train Epoch: 4/30 [44800/269796 (17%)]	Training loss for batch 701/4216: 0.076469
Train Epoch: 4/30 [51200/269796 (19%)]	Training loss for batch 801/4216: 0.064214
Train Epoch: 4/30 [57600/269796 (21%)]	Training loss for batch 901/4216: 0.059579

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0640, Precision: 75.58%, Recall: 34.25%, F1: 46.35%, Acc: 98.06%

Train Epoch: 4/30 [64000/269796 (24%)]	Training loss for batch 1001/4216: 0.060317
Train Epoch: 4/30 [70400/269796 (26%)]	Training loss for batch 1101/4216: 0.069990
Train Epoch: 4/30 [76800/269796 (28%)]	Training loss for batch 1201/4216: 0.058488
Train Epoch: 4/30 [83200/269796 (31%)]	Training loss for batch 1301/4216: 0.063994
Train Epoch: 4/30 [89600/269796 (33%)]	Training loss for batch 1401/4216: 0.056613
Train Epoch: 4/30 [96000/269796 (36%)]	Training loss for batch 1501/4216: 0.066566
Train Epoch: 4/30 [102400/269796 (38%)]	Training loss for batch 1601/4216: 0.067762
Train Epoch: 4/30 [108800/269796 (40%)]	Training loss for batch 1701/4216: 0.064165
Train Epoch: 4/30 [115200/269796 (43%)]	Training loss for batch 1801/4216: 0.059348
Train Epoch: 4/30 [121600/269796 (45%)]	Training loss for batch 1901/4216: 0.067786

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0629, Precision: 75.25%, Recall: 35.50%, F1: 47.54%, Acc: 98.08%

Train Epoch: 4/30 [128000/269796 (47%)]	Training loss for batch 2001/4216: 0.068318
Train Epoch: 4/30 [134400/269796 (50%)]	Training loss for batch 2101/4216: 0.058135
Train Epoch: 4/30 [140800/269796 (52%)]	Training loss for batch 2201/4216: 0.063398
Train Epoch: 4/30 [147200/269796 (55%)]	Training loss for batch 2301/4216: 0.069980
Train Epoch: 4/30 [153600/269796 (57%)]	Training loss for batch 2401/4216: 0.067256
Train Epoch: 4/30 [160000/269796 (59%)]	Training loss for batch 2501/4216: 0.057276
Train Epoch: 4/30 [166400/269796 (62%)]	Training loss for batch 2601/4216: 0.054346
Train Epoch: 4/30 [172800/269796 (64%)]	Training loss for batch 2701/4216: 0.054867
Train Epoch: 4/30 [179200/269796 (66%)]	Training loss for batch 2801/4216: 0.046628
Train Epoch: 4/30 [185600/269796 (69%)]	Training loss for batch 2901/4216: 0.073545

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0615, Precision: 74.14%, Recall: 38.66%, F1: 50.24%, Acc: 98.11%

Train Epoch: 4/30 [192000/269796 (71%)]	Training loss for batch 3001/4216: 0.066357
Train Epoch: 4/30 [198400/269796 (74%)]	Training loss for batch 3101/4216: 0.063281
Train Epoch: 4/30 [204800/269796 (76%)]	Training loss for batch 3201/4216: 0.055421
Train Epoch: 4/30 [211200/269796 (78%)]	Training loss for batch 3301/4216: 0.070114
Train Epoch: 4/30 [217600/269796 (81%)]	Training loss for batch 3401/4216: 0.079132
Train Epoch: 4/30 [224000/269796 (83%)]	Training loss for batch 3501/4216: 0.064987
Train Epoch: 4/30 [230400/269796 (85%)]	Training loss for batch 3601/4216: 0.065211
Train Epoch: 4/30 [236800/269796 (88%)]	Training loss for batch 3701/4216: 0.061016
Train Epoch: 4/30 [243200/269796 (90%)]	Training loss for batch 3801/4216: 0.059853
Train Epoch: 4/30 [249600/269796 (93%)]	Training loss for batch 3901/4216: 0.064940

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0615, Precision: 73.46%, Recall: 41.21%, F1: 52.32%, Acc: 98.14%

Train Epoch: 4/30 [256000/269796 (95%)]	Training loss for batch 4001/4216: 0.063444
Train Epoch: 4/30 [262400/269796 (97%)]	Training loss for batch 4101/4216: 0.066671
Train Epoch: 4/30 [268800/269796 (100%)]	Training loss for batch 4201/4216: 0.065937

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0605, Precision: 74.94%, Recall: 39.75%, F1: 51.37%, Acc: 98.14%

Train Epoch: 5/30 [0/269796 (0%)]	Training loss for batch 1/4216: 0.072832
Train Epoch: 5/30 [6400/269796 (2%)]	Training loss for batch 101/4216: 0.060151
Train Epoch: 5/30 [12800/269796 (5%)]	Training loss for batch 201/4216: 0.063095
Train Epoch: 5/30 [19200/269796 (7%)]	Training loss for batch 301/4216: 0.061953
Train Epoch: 5/30 [25600/269796 (9%)]	Training loss for batch 401/4216: 0.063835
Train Epoch: 5/30 [32000/269796 (12%)]	Training loss for batch 501/4216: 0.057656
Train Epoch: 5/30 [38400/269796 (14%)]	Training loss for batch 601/4216: 0.055821
Train Epoch: 5/30 [44800/269796 (17%)]	Training loss for batch 701/4216: 0.056281
Train Epoch: 5/30 [51200/269796 (19%)]	Training loss for batch 801/4216: 0.065850
Train Epoch: 5/30 [57600/269796 (21%)]	Training loss for batch 901/4216: 0.065779

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0614, Precision: 75.43%, Recall: 35.93%, F1: 48.14%, Acc: 98.09%

Train Epoch: 5/30 [64000/269796 (24%)]	Training loss for batch 1001/4216: 0.076000
Train Epoch: 5/30 [70400/269796 (26%)]	Training loss for batch 1101/4216: 0.057565
Train Epoch: 5/30 [76800/269796 (28%)]	Training loss for batch 1201/4216: 0.054314
Train Epoch: 5/30 [83200/269796 (31%)]	Training loss for batch 1301/4216: 0.061979
Train Epoch: 5/30 [89600/269796 (33%)]	Training loss for batch 1401/4216: 0.057118
Train Epoch: 5/30 [96000/269796 (36%)]	Training loss for batch 1501/4216: 0.064464
Train Epoch: 5/30 [102400/269796 (38%)]	Training loss for batch 1601/4216: 0.060388
Train Epoch: 5/30 [108800/269796 (40%)]	Training loss for batch 1701/4216: 0.058536
Train Epoch: 5/30 [115200/269796 (43%)]	Training loss for batch 1801/4216: 0.048825
Train Epoch: 5/30 [121600/269796 (45%)]	Training loss for batch 1901/4216: 0.048511

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0597, Precision: 75.43%, Recall: 39.73%, F1: 51.49%, Acc: 98.15%

Train Epoch: 5/30 [128000/269796 (47%)]	Training loss for batch 2001/4216: 0.068737
Train Epoch: 5/30 [134400/269796 (50%)]	Training loss for batch 2101/4216: 0.063666
Train Epoch: 5/30 [140800/269796 (52%)]	Training loss for batch 2201/4216: 0.067972
Train Epoch: 5/30 [147200/269796 (55%)]	Training loss for batch 2301/4216: 0.052089
Train Epoch: 5/30 [153600/269796 (57%)]	Training loss for batch 2401/4216: 0.050983
Train Epoch: 5/30 [160000/269796 (59%)]	Training loss for batch 2501/4216: 0.060684
Train Epoch: 5/30 [166400/269796 (62%)]	Training loss for batch 2601/4216: 0.059869
Train Epoch: 5/30 [172800/269796 (64%)]	Training loss for batch 2701/4216: 0.055935
Train Epoch: 5/30 [179200/269796 (66%)]	Training loss for batch 2801/4216: 0.064215
Train Epoch: 5/30 [185600/269796 (69%)]	Training loss for batch 2901/4216: 0.059526

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0595, Precision: 75.39%, Recall: 39.94%, F1: 51.65%, Acc: 98.16%

Train Epoch: 5/30 [192000/269796 (71%)]	Training loss for batch 3001/4216: 0.069829
Train Epoch: 5/30 [198400/269796 (74%)]	Training loss for batch 3101/4216: 0.069687
Train Epoch: 5/30 [204800/269796 (76%)]	Training loss for batch 3201/4216: 0.054200
Train Epoch: 5/30 [211200/269796 (78%)]	Training loss for batch 3301/4216: 0.059163
Train Epoch: 5/30 [217600/269796 (81%)]	Training loss for batch 3401/4216: 0.067482
Train Epoch: 5/30 [224000/269796 (83%)]	Training loss for batch 3501/4216: 0.064750
Train Epoch: 5/30 [230400/269796 (85%)]	Training loss for batch 3601/4216: 0.061504
Train Epoch: 5/30 [236800/269796 (88%)]	Training loss for batch 3701/4216: 0.069141
Train Epoch: 5/30 [243200/269796 (90%)]	Training loss for batch 3801/4216: 0.047989
Train Epoch: 5/30 [249600/269796 (93%)]	Training loss for batch 3901/4216: 0.059271

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0585, Precision: 74.39%, Recall: 43.11%, F1: 54.13%, Acc: 98.19%

Train Epoch: 5/30 [256000/269796 (95%)]	Training loss for batch 4001/4216: 0.045197
Train Epoch: 5/30 [262400/269796 (97%)]	Training loss for batch 4101/4216: 0.041835
Train Epoch: 5/30 [268800/269796 (100%)]	Training loss for batch 4201/4216: 0.067177

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0583, Precision: 74.81%, Recall: 42.64%, F1: 53.85%, Acc: 98.19%

Train Epoch: 6/30 [0/269796 (0%)]	Training loss for batch 1/4216: 0.069460
Train Epoch: 6/30 [6400/269796 (2%)]	Training loss for batch 101/4216: 0.056075
Train Epoch: 6/30 [12800/269796 (5%)]	Training loss for batch 201/4216: 0.057598
Train Epoch: 6/30 [19200/269796 (7%)]	Training loss for batch 301/4216: 0.053617
Train Epoch: 6/30 [25600/269796 (9%)]	Training loss for batch 401/4216: 0.057107
Train Epoch: 6/30 [32000/269796 (12%)]	Training loss for batch 501/4216: 0.058144
Train Epoch: 6/30 [38400/269796 (14%)]	Training loss for batch 601/4216: 0.056452
Train Epoch: 6/30 [44800/269796 (17%)]	Training loss for batch 701/4216: 0.055390
Train Epoch: 6/30 [51200/269796 (19%)]	Training loss for batch 801/4216: 0.055185
Train Epoch: 6/30 [57600/269796 (21%)]	Training loss for batch 901/4216: 0.067181

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0587, Precision: 75.38%, Recall: 40.97%, F1: 52.53%, Acc: 98.17%

Train Epoch: 6/30 [64000/269796 (24%)]	Training loss for batch 1001/4216: 0.057531
Train Epoch: 6/30 [70400/269796 (26%)]	Training loss for batch 1101/4216: 0.065399
Train Epoch: 6/30 [76800/269796 (28%)]	Training loss for batch 1201/4216: 0.053327
Train Epoch: 6/30 [83200/269796 (31%)]	Training loss for batch 1301/4216: 0.054724
Train Epoch: 6/30 [89600/269796 (33%)]	Training loss for batch 1401/4216: 0.065021
Train Epoch: 6/30 [96000/269796 (36%)]	Training loss for batch 1501/4216: 0.062672
Train Epoch: 6/30 [102400/269796 (38%)]	Training loss for batch 1601/4216: 0.061608
Train Epoch: 6/30 [108800/269796 (40%)]	Training loss for batch 1701/4216: 0.054095
Train Epoch: 6/30 [115200/269796 (43%)]	Training loss for batch 1801/4216: 0.068202
Train Epoch: 6/30 [121600/269796 (45%)]	Training loss for batch 1901/4216: 0.068121

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0576, Precision: 74.94%, Recall: 43.42%, F1: 54.54%, Acc: 98.21%

Train Epoch: 6/30 [128000/269796 (47%)]	Training loss for batch 2001/4216: 0.064838
Train Epoch: 6/30 [134400/269796 (50%)]	Training loss for batch 2101/4216: 0.050784
Train Epoch: 6/30 [140800/269796 (52%)]	Training loss for batch 2201/4216: 0.063540
Train Epoch: 6/30 [147200/269796 (55%)]	Training loss for batch 2301/4216: 0.049249
Train Epoch: 6/30 [153600/269796 (57%)]	Training loss for batch 2401/4216: 0.050116
Train Epoch: 6/30 [160000/269796 (59%)]	Training loss for batch 2501/4216: 0.055535
Train Epoch: 6/30 [166400/269796 (62%)]	Training loss for batch 2601/4216: 0.065385
Train Epoch: 6/30 [172800/269796 (64%)]	Training loss for batch 2701/4216: 0.058288
Train Epoch: 6/30 [179200/269796 (66%)]	Training loss for batch 2801/4216: 0.052554
Train Epoch: 6/30 [185600/269796 (69%)]	Training loss for batch 2901/4216: 0.057982

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0570, Precision: 75.51%, Recall: 43.68%, F1: 54.89%, Acc: 98.23%

Train Epoch: 6/30 [192000/269796 (71%)]	Training loss for batch 3001/4216: 0.052763
Train Epoch: 6/30 [198400/269796 (74%)]	Training loss for batch 3101/4216: 0.062766
Train Epoch: 6/30 [204800/269796 (76%)]	Training loss for batch 3201/4216: 0.057373
Train Epoch: 6/30 [211200/269796 (78%)]	Training loss for batch 3301/4216: 0.059387
Train Epoch: 6/30 [217600/269796 (81%)]	Training loss for batch 3401/4216: 0.056976
Train Epoch: 6/30 [224000/269796 (83%)]	Training loss for batch 3501/4216: 0.065625
Train Epoch: 6/30 [230400/269796 (85%)]	Training loss for batch 3601/4216: 0.053116
Train Epoch: 6/30 [236800/269796 (88%)]	Training loss for batch 3701/4216: 0.052597
Train Epoch: 6/30 [243200/269796 (90%)]	Training loss for batch 3801/4216: 0.061322
Train Epoch: 6/30 [249600/269796 (93%)]	Training loss for batch 3901/4216: 0.050884

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0569, Precision: 75.69%, Recall: 43.95%, F1: 55.14%, Acc: 98.23%

Train Epoch: 6/30 [256000/269796 (95%)]	Training loss for batch 4001/4216: 0.058826
Train Epoch: 6/30 [262400/269796 (97%)]	Training loss for batch 4101/4216: 0.059317
Train Epoch: 6/30 [268800/269796 (100%)]	Training loss for batch 4201/4216: 0.065896

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0576, Precision: 73.60%, Recall: 45.96%, F1: 56.22%, Acc: 98.22%

Train Epoch: 7/30 [0/269796 (0%)]	Training loss for batch 1/4216: 0.064051
Train Epoch: 7/30 [6400/269796 (2%)]	Training loss for batch 101/4216: 0.043766
Train Epoch: 7/30 [12800/269796 (5%)]	Training loss for batch 201/4216: 0.052941
Train Epoch: 7/30 [19200/269796 (7%)]	Training loss for batch 301/4216: 0.054409
Train Epoch: 7/30 [25600/269796 (9%)]	Training loss for batch 401/4216: 0.064312
Train Epoch: 7/30 [32000/269796 (12%)]	Training loss for batch 501/4216: 0.053571
Train Epoch: 7/30 [38400/269796 (14%)]	Training loss for batch 601/4216: 0.058419
Train Epoch: 7/30 [44800/269796 (17%)]	Training loss for batch 701/4216: 0.041052
Train Epoch: 7/30 [51200/269796 (19%)]	Training loss for batch 801/4216: 0.063550
Train Epoch: 7/30 [57600/269796 (21%)]	Training loss for batch 901/4216: 0.057794

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0576, Precision: 74.12%, Recall: 43.83%, F1: 54.65%, Acc: 98.20%

Train Epoch: 7/30 [64000/269796 (24%)]	Training loss for batch 1001/4216: 0.053744
Train Epoch: 7/30 [70400/269796 (26%)]	Training loss for batch 1101/4216: 0.047799
Train Epoch: 7/30 [76800/269796 (28%)]	Training loss for batch 1201/4216: 0.056097
Train Epoch: 7/30 [83200/269796 (31%)]	Training loss for batch 1301/4216: 0.059431
Train Epoch: 7/30 [89600/269796 (33%)]	Training loss for batch 1401/4216: 0.059778
Train Epoch: 7/30 [96000/269796 (36%)]	Training loss for batch 1501/4216: 0.047946
Train Epoch: 7/30 [102400/269796 (38%)]	Training loss for batch 1601/4216: 0.052649
Train Epoch: 7/30 [108800/269796 (40%)]	Training loss for batch 1701/4216: 0.057225
Train Epoch: 7/30 [115200/269796 (43%)]	Training loss for batch 1801/4216: 0.056025
Train Epoch: 7/30 [121600/269796 (45%)]	Training loss for batch 1901/4216: 0.064514

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0566, Precision: 74.76%, Recall: 46.15%, F1: 56.69%, Acc: 98.26%

Train Epoch: 7/30 [128000/269796 (47%)]	Training loss for batch 2001/4216: 0.046220
Train Epoch: 7/30 [134400/269796 (50%)]	Training loss for batch 2101/4216: 0.051373
Train Epoch: 7/30 [140800/269796 (52%)]	Training loss for batch 2201/4216: 0.048926
Train Epoch: 7/30 [147200/269796 (55%)]	Training loss for batch 2301/4216: 0.044127
Train Epoch: 7/30 [153600/269796 (57%)]	Training loss for batch 2401/4216: 0.051037
Train Epoch: 7/30 [160000/269796 (59%)]	Training loss for batch 2501/4216: 0.051606
Train Epoch: 7/30 [166400/269796 (62%)]	Training loss for batch 2601/4216: 0.047616
Train Epoch: 7/30 [172800/269796 (64%)]	Training loss for batch 2701/4216: 0.046308
Train Epoch: 7/30 [179200/269796 (66%)]	Training loss for batch 2801/4216: 0.046298
Train Epoch: 7/30 [185600/269796 (69%)]	Training loss for batch 2901/4216: 0.053418

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0559, Precision: 73.62%, Recall: 48.34%, F1: 58.06%, Acc: 98.27%

Train Epoch: 7/30 [192000/269796 (71%)]	Training loss for batch 3001/4216: 0.065501
Train Epoch: 7/30 [198400/269796 (74%)]	Training loss for batch 3101/4216: 0.068695
Train Epoch: 7/30 [204800/269796 (76%)]	Training loss for batch 3201/4216: 0.068426
Train Epoch: 7/30 [211200/269796 (78%)]	Training loss for batch 3301/4216: 0.057939
Train Epoch: 7/30 [217600/269796 (81%)]	Training loss for batch 3401/4216: 0.056468
Train Epoch: 7/30 [224000/269796 (83%)]	Training loss for batch 3501/4216: 0.053309
Train Epoch: 7/30 [230400/269796 (85%)]	Training loss for batch 3601/4216: 0.063508
Train Epoch: 7/30 [236800/269796 (88%)]	Training loss for batch 3701/4216: 0.052598
Train Epoch: 7/30 [243200/269796 (90%)]	Training loss for batch 3801/4216: 0.048617
Train Epoch: 7/30 [249600/269796 (93%)]	Training loss for batch 3901/4216: 0.051342

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0559, Precision: 75.07%, Recall: 45.57%, F1: 56.33%, Acc: 98.25%

Train Epoch: 7/30 [256000/269796 (95%)]	Training loss for batch 4001/4216: 0.053031
Train Epoch: 7/30 [262400/269796 (97%)]	Training loss for batch 4101/4216: 0.064155
Train Epoch: 7/30 [268800/269796 (100%)]	Training loss for batch 4201/4216: 0.047016

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0559, Precision: 74.31%, Recall: 46.74%, F1: 57.01%, Acc: 98.26%

Train Epoch: 8/30 [0/269796 (0%)]	Training loss for batch 1/4216: 0.053425
Train Epoch: 8/30 [6400/269796 (2%)]	Training loss for batch 101/4216: 0.063853
Train Epoch: 8/30 [12800/269796 (5%)]	Training loss for batch 201/4216: 0.057617
Train Epoch: 8/30 [19200/269796 (7%)]	Training loss for batch 301/4216: 0.062133
Train Epoch: 8/30 [25600/269796 (9%)]	Training loss for batch 401/4216: 0.058608
Train Epoch: 8/30 [32000/269796 (12%)]	Training loss for batch 501/4216: 0.070658
Train Epoch: 8/30 [38400/269796 (14%)]	Training loss for batch 601/4216: 0.048275
Train Epoch: 8/30 [44800/269796 (17%)]	Training loss for batch 701/4216: 0.062550
Train Epoch: 8/30 [51200/269796 (19%)]	Training loss for batch 801/4216: 0.048678
Train Epoch: 8/30 [57600/269796 (21%)]	Training loss for batch 901/4216: 0.062172

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0562, Precision: 75.94%, Recall: 43.32%, F1: 54.71%, Acc: 98.24%

Train Epoch: 8/30 [64000/269796 (24%)]	Training loss for batch 1001/4216: 0.067604
Train Epoch: 8/30 [70400/269796 (26%)]	Training loss for batch 1101/4216: 0.049972
Train Epoch: 8/30 [76800/269796 (28%)]	Training loss for batch 1201/4216: 0.048102
Train Epoch: 8/30 [83200/269796 (31%)]	Training loss for batch 1301/4216: 0.052052
Train Epoch: 8/30 [89600/269796 (33%)]	Training loss for batch 1401/4216: 0.050236
Train Epoch: 8/30 [96000/269796 (36%)]	Training loss for batch 1501/4216: 0.055135
Train Epoch: 8/30 [102400/269796 (38%)]	Training loss for batch 1601/4216: 0.047155
Train Epoch: 8/30 [108800/269796 (40%)]	Training loss for batch 1701/4216: 0.040704
Train Epoch: 8/30 [115200/269796 (43%)]	Training loss for batch 1801/4216: 0.051996
Train Epoch: 8/30 [121600/269796 (45%)]	Training loss for batch 1901/4216: 0.059052

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0562, Precision: 72.81%, Recall: 48.91%, F1: 58.25%, Acc: 98.27%

Train Epoch: 8/30 [128000/269796 (47%)]	Training loss for batch 2001/4216: 0.044763
Train Epoch: 8/30 [134400/269796 (50%)]	Training loss for batch 2101/4216: 0.051622
Train Epoch: 8/30 [140800/269796 (52%)]	Training loss for batch 2201/4216: 0.041754
Train Epoch: 8/30 [147200/269796 (55%)]	Training loss for batch 2301/4216: 0.051248
Train Epoch: 8/30 [153600/269796 (57%)]	Training loss for batch 2401/4216: 0.066904
Train Epoch: 8/30 [160000/269796 (59%)]	Training loss for batch 2501/4216: 0.056838
Train Epoch: 8/30 [166400/269796 (62%)]	Training loss for batch 2601/4216: 0.045060
Train Epoch: 8/30 [172800/269796 (64%)]	Training loss for batch 2701/4216: 0.049307
Train Epoch: 8/30 [179200/269796 (66%)]	Training loss for batch 2801/4216: 0.049922
Train Epoch: 8/30 [185600/269796 (69%)]	Training loss for batch 2901/4216: 0.053354

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0553, Precision: 74.95%, Recall: 47.01%, F1: 57.42%, Acc: 98.28%

Train Epoch: 8/30 [192000/269796 (71%)]	Training loss for batch 3001/4216: 0.060702
Train Epoch: 8/30 [198400/269796 (74%)]	Training loss for batch 3101/4216: 0.039869
Train Epoch: 8/30 [204800/269796 (76%)]	Training loss for batch 3201/4216: 0.054914
Train Epoch: 8/30 [211200/269796 (78%)]	Training loss for batch 3301/4216: 0.047057
Train Epoch: 8/30 [217600/269796 (81%)]	Training loss for batch 3401/4216: 0.059895
Train Epoch: 8/30 [224000/269796 (83%)]	Training loss for batch 3501/4216: 0.042538
Train Epoch: 8/30 [230400/269796 (85%)]	Training loss for batch 3601/4216: 0.058387
Train Epoch: 8/30 [236800/269796 (88%)]	Training loss for batch 3701/4216: 0.053881
Train Epoch: 8/30 [243200/269796 (90%)]	Training loss for batch 3801/4216: 0.048981
Train Epoch: 8/30 [249600/269796 (93%)]	Training loss for batch 3901/4216: 0.055164

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0551, Precision: 74.42%, Recall: 48.21%, F1: 58.19%, Acc: 98.29%

Train Epoch: 8/30 [256000/269796 (95%)]	Training loss for batch 4001/4216: 0.042695
Train Epoch: 8/30 [262400/269796 (97%)]	Training loss for batch 4101/4216: 0.058309
Train Epoch: 8/30 [268800/269796 (100%)]	Training loss for batch 4201/4216: 0.052028

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0553, Precision: 73.76%, Recall: 48.56%, F1: 58.26%, Acc: 98.28%

Train Epoch: 9/30 [0/269796 (0%)]	Training loss for batch 1/4216: 0.046721
Train Epoch: 9/30 [6400/269796 (2%)]	Training loss for batch 101/4216: 0.062611
Train Epoch: 9/30 [12800/269796 (5%)]	Training loss for batch 201/4216: 0.055967
Train Epoch: 9/30 [19200/269796 (7%)]	Training loss for batch 301/4216: 0.062304
Train Epoch: 9/30 [25600/269796 (9%)]	Training loss for batch 401/4216: 0.052465
Train Epoch: 9/30 [32000/269796 (12%)]	Training loss for batch 501/4216: 0.049156
Train Epoch: 9/30 [38400/269796 (14%)]	Training loss for batch 601/4216: 0.056599
Train Epoch: 9/30 [44800/269796 (17%)]	Training loss for batch 701/4216: 0.041421
Train Epoch: 9/30 [51200/269796 (19%)]	Training loss for batch 801/4216: 0.054909
Train Epoch: 9/30 [57600/269796 (21%)]	Training loss for batch 901/4216: 0.082236

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0561, Precision: 75.23%, Recall: 44.05%, F1: 55.12%, Acc: 98.24%

Train Epoch: 9/30 [64000/269796 (24%)]	Training loss for batch 1001/4216: 0.060821
Train Epoch: 9/30 [70400/269796 (26%)]	Training loss for batch 1101/4216: 0.060163
Train Epoch: 9/30 [76800/269796 (28%)]	Training loss for batch 1201/4216: 0.050820
Train Epoch: 9/30 [83200/269796 (31%)]	Training loss for batch 1301/4216: 0.050427
Train Epoch: 9/30 [89600/269796 (33%)]	Training loss for batch 1401/4216: 0.043956
Train Epoch: 9/30 [96000/269796 (36%)]	Training loss for batch 1501/4216: 0.057367
Train Epoch: 9/30 [102400/269796 (38%)]	Training loss for batch 1601/4216: 0.049432
Train Epoch: 9/30 [108800/269796 (40%)]	Training loss for batch 1701/4216: 0.050455
Train Epoch: 9/30 [115200/269796 (43%)]	Training loss for batch 1801/4216: 0.048917
Train Epoch: 9/30 [121600/269796 (45%)]	Training loss for batch 1901/4216: 0.063949

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0550, Precision: 74.43%, Recall: 48.39%, F1: 58.34%, Acc: 98.29%

Train Epoch: 9/30 [128000/269796 (47%)]	Training loss for batch 2001/4216: 0.046645
Train Epoch: 9/30 [134400/269796 (50%)]	Training loss for batch 2101/4216: 0.054924
Train Epoch: 9/30 [140800/269796 (52%)]	Training loss for batch 2201/4216: 0.061758
Train Epoch: 9/30 [147200/269796 (55%)]	Training loss for batch 2301/4216: 0.051688
Train Epoch: 9/30 [153600/269796 (57%)]	Training loss for batch 2401/4216: 0.047575
Train Epoch: 9/30 [160000/269796 (59%)]	Training loss for batch 2501/4216: 0.048276
Train Epoch: 9/30 [166400/269796 (62%)]	Training loss for batch 2601/4216: 0.061343
Train Epoch: 9/30 [172800/269796 (64%)]	Training loss for batch 2701/4216: 0.051554
Train Epoch: 9/30 [179200/269796 (66%)]	Training loss for batch 2801/4216: 0.053405
Train Epoch: 9/30 [185600/269796 (69%)]	Training loss for batch 2901/4216: 0.046954

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0550, Precision: 75.10%, Recall: 47.47%, F1: 57.81%, Acc: 98.30%

Train Epoch: 9/30 [192000/269796 (71%)]	Training loss for batch 3001/4216: 0.060259
Train Epoch: 9/30 [198400/269796 (74%)]	Training loss for batch 3101/4216: 0.057518
Train Epoch: 9/30 [204800/269796 (76%)]	Training loss for batch 3201/4216: 0.050654
Train Epoch: 9/30 [211200/269796 (78%)]	Training loss for batch 3301/4216: 0.055715
Train Epoch: 9/30 [217600/269796 (81%)]	Training loss for batch 3401/4216: 0.042933
Train Epoch: 9/30 [224000/269796 (83%)]	Training loss for batch 3501/4216: 0.046836
Train Epoch: 9/30 [230400/269796 (85%)]	Training loss for batch 3601/4216: 0.058464
Train Epoch: 9/30 [236800/269796 (88%)]	Training loss for batch 3701/4216: 0.060386
Train Epoch: 9/30 [243200/269796 (90%)]	Training loss for batch 3801/4216: 0.056364
Train Epoch: 9/30 [249600/269796 (93%)]	Training loss for batch 3901/4216: 0.055379

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0548, Precision: 74.19%, Recall: 48.85%, F1: 58.60%, Acc: 98.29%

Train Epoch: 9/30 [256000/269796 (95%)]	Training loss for batch 4001/4216: 0.054202
Train Epoch: 9/30 [262400/269796 (97%)]	Training loss for batch 4101/4216: 0.043979
Train Epoch: 9/30 [268800/269796 (100%)]	Training loss for batch 4201/4216: 0.051156

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0549, Precision: 73.51%, Recall: 50.28%, F1: 59.45%, Acc: 98.31%

Train Epoch: 10/30 [0/269796 (0%)]	Training loss for batch 1/4216: 0.056736
Train Epoch: 10/30 [6400/269796 (2%)]	Training loss for batch 101/4216: 0.066706
Train Epoch: 10/30 [12800/269796 (5%)]	Training loss for batch 201/4216: 0.057115
Train Epoch: 10/30 [19200/269796 (7%)]	Training loss for batch 301/4216: 0.051706
Train Epoch: 10/30 [25600/269796 (9%)]	Training loss for batch 401/4216: 0.050834
Train Epoch: 10/30 [32000/269796 (12%)]	Training loss for batch 501/4216: 0.066093
Train Epoch: 10/30 [38400/269796 (14%)]	Training loss for batch 601/4216: 0.040089
Train Epoch: 10/30 [44800/269796 (17%)]	Training loss for batch 701/4216: 0.055696
Train Epoch: 10/30 [51200/269796 (19%)]	Training loss for batch 801/4216: 0.048754
Train Epoch: 10/30 [57600/269796 (21%)]	Training loss for batch 901/4216: 0.059449

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0545, Precision: 74.73%, Recall: 48.85%, F1: 58.76%, Acc: 98.31%

Train Epoch: 10/30 [64000/269796 (24%)]	Training loss for batch 1001/4216: 0.050763
Train Epoch: 10/30 [70400/269796 (26%)]	Training loss for batch 1101/4216: 0.050174
Train Epoch: 10/30 [76800/269796 (28%)]	Training loss for batch 1201/4216: 0.056472
Train Epoch: 10/30 [83200/269796 (31%)]	Training loss for batch 1301/4216: 0.053305
Train Epoch: 10/30 [89600/269796 (33%)]	Training loss for batch 1401/4216: 0.058209
Train Epoch: 10/30 [96000/269796 (36%)]	Training loss for batch 1501/4216: 0.052735
Train Epoch: 10/30 [102400/269796 (38%)]	Training loss for batch 1601/4216: 0.035854
Train Epoch: 10/30 [108800/269796 (40%)]	Training loss for batch 1701/4216: 0.042560
Train Epoch: 10/30 [115200/269796 (43%)]	Training loss for batch 1801/4216: 0.043340
Train Epoch: 10/30 [121600/269796 (45%)]	Training loss for batch 1901/4216: 0.055811

Dev set scores for model Doc-enc=w2v-nl4-ks20x5+20x5+20x2+20x2-st1x1+1x1+1x1+1x1-ps2x2+2x2+2x2+1x9-di1x1+1x1+1x1+1x1-pd0x0+0x0+0x0+0x0-in300x100-nk25+50+100+100-rel-rel-sig-d0.2-h100-bs64-ep30-ada-default-bce.pt:
Average loss: 0.0548, Precision: 73.90%, Recall: 49.67%, F1: 59.11%, Acc: 98.30%

Train Epoch: 10/30 [128000/269796 (47%)]	Training loss for batch 2001/4216: 0.049253
Train Epoch: 10/30 [134400/269796 (50%)]	Training loss for batch 2101/4216: 0.041879
Train Epoch: 10/30 [140800/269796 (52%)]	Training loss for batch 2201/4216: 0.054989
Train Epoch: 10/30 [147200/269796 (55%)]	Training loss for batch 2301/4216: 0.064494
Train Epoch: 10/30 [153600/269796 (57%)]	Training loss for batch 2401/4216: 0.046020
Train Epoch: 10/30 [160000/269796 (59%)]	Training loss for batch 2501/4216: 0.062902
Train Epoch: 10/30 [166400/269796 (62%)]	Training loss for batch 2601/4216: 0.052909
Train Epoch: 10/30 [172800/269796 (64%)]	Training loss for batch 2701/4216: 0.041418
Train Epoch: 10/30 [179200/269796 (66%)]	Training loss for batch 2801/4216: 0.050288
Train Epoch: 10/30 [185600/269796 (69%)]	Training loss for batch 2901/4216: 0.061791
